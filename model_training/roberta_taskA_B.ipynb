{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import AdamW"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:36.994207Z",
     "start_time": "2023-12-10T13:29:34.954757Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Further Pretraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do further pretraining from gab_1M_unlaelled and reddit_1M_unlabelled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import emoji\n",
    "# from tqdm import tqdm\n",
    "# df_gab = pd.read_csv('data/gab_1M_unlabelled.csv')\n",
    "# df_reddit = pd.read_csv('data/reddit_1M_unlabelled.csv')\n",
    "# further_corpus =further_corpus [emoji.demojize(text, delimiters=('', ''), language= 'en').replace('_',' ') for text in tqdm(df_gab['text'].tolist() + df_reddit['text'].tolist())]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T11:03:27.353804Z",
     "start_time": "2023-12-10T11:03:27.352135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# with open('data/further_corpus.txt', 'w') as file:\n",
    "#     # Write each element of the list to the file\n",
    "#     for item in further_corpus:\n",
    "#         file.write(f\"{item}\\n\")  # Add a newline after each item"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T11:03:27.359908Z",
     "start_time": "2023-12-10T11:03:27.354202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.shuffle(further_corpus)\n",
    "# split_index = int(len(further_corpus) * 0.8)\n",
    "# further_corpus_train = further_corpus[:split_index]\n",
    "# further_corpus_eval = further_corpus[split_index:]\n",
    "# \n",
    "# with open('data/further_corpus_train.txt', 'w') as file:\n",
    "#     # Write each element of the list to the file\n",
    "#     for item in further_corpus_train:\n",
    "#         file.write(f\"{item}\\n\")  # Add a newline after each item\n",
    "# \n",
    "# with open('data/further_corpus_eval.txt', 'w') as file:\n",
    "#     # Write each element of the list to the file\n",
    "#     for item in further_corpus_eval:\n",
    "#         file.write(f\"{item}\\n\")  # Add a newline after each item"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T11:03:27.360205Z",
     "start_time": "2023-12-10T11:03:27.356338Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "python run_mlm.py \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --train_file data/further_corpus_train.txt \\\n",
    "    --validation_file data/further_corpus_eval.txt \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --line_by_line \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir tmp/test-mlm \n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# lode the texts and labels\n",
    "df = pd.read_csv('data/edos_cleaned.csv')"
   ],
   "metadata": {
    "id": "DBrSru0z7gzV",
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:40.156827Z",
     "start_time": "2023-12-10T13:29:39.812391Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_texts_binary = df[df['split'] == 'train']['text_en_emoji_rm_url'].tolist()\n",
    "train_labels_binary = [1 if (label == \"sexist\") else 0 for label in df[df['split'] == 'train']['label_sexist'].tolist()]\n",
    "\n",
    "dev_texts_binary = df[df['split'] == 'dev']['text_en_emoji_rm_url'].tolist()\n",
    "dev_labels_binary = [1 if (label == \"sexist\") else 0 for label in df[df['split'] == 'dev']['label_sexist'].tolist()]\n",
    "\n",
    "test_texts_binary = df[df['split'] == 'test']['text_en_emoji_rm_url'].tolist()\n",
    "test_labels_binary = [1 if (label == \"sexist\") else 0 for label in df[df['split'] == 'test']['label_sexist'].tolist()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:41.393224Z",
     "start_time": "2023-12-10T13:29:41.372648Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_sexist = df[df['label_sexist'] == 'sexist']\n",
    "categories ={\n",
    "'1. threats, plans to harm and incitement': 0,\n",
    "'2. derogation': 1,\n",
    "'3. animosity': 2,\n",
    "'4. prejudiced discussions': 3\n",
    "}\n",
    "\n",
    "train_texts_four_categories = df_sexist[df_sexist['split'] == 'train']['text_en_emoji_rm_url'].tolist()\n",
    "train_labels_four_categories = list(map(lambda x: categories[x] if x in categories else x, df_sexist[df_sexist['split'] == 'train']['label_category'].tolist()))\n",
    "\n",
    "dev_texts_four_categories = df_sexist[df_sexist['split'] == 'dev']['text_en_emoji_rm_url'].tolist()\n",
    "dev_labels_four_categories = list(map(lambda x: categories[x] if x in categories else x, df_sexist[df_sexist['split'] == 'dev']['label_category'].tolist()))\n",
    "\n",
    "test_texts_four_categories = df_sexist[df_sexist['split'] == 'test']['text_en_emoji_rm_url'].tolist()\n",
    "test_labels_four_categories = list(map(lambda x: categories[x] if x in categories else x, df_sexist[df_sexist['split'] == 'test']['label_category'].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:43.440539Z",
     "start_time": "2023-12-10T13:29:43.431926Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
   ],
   "metadata": {
    "id": "g251hIrU1yM4",
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:45.139116Z",
     "start_time": "2023-12-10T13:29:45.134097Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class RobertaClassifier(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_classes):\n",
    "        super(RobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        droupout_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(droupout_output)\n",
    "        return logits\n",
    "    \n",
    "# class RobertaMulti(nn.Module):\n",
    "#     def __init__(self, roberta_model_name, num_classes):\n",
    "#         super(RobertaMulti, self).__init__()\n",
    "#         self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.linear = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "#         # self.relu = nn.ReLU()\n",
    "# \n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = outputs.pooler_output\n",
    "#         droupout_output = self.dropout(pooled_output)\n",
    "#         logits = self.linear(droupout_output)\n",
    "#         return logits"
   ],
   "metadata": {
    "id": "E7BC_lx73ISo",
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:46.434972Z",
     "start_time": "2023-12-10T13:29:46.430800Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print(f\"Batch Loss: {loss.item():.4f}\")\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(data_loader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")"
   ],
   "metadata": {
    "id": "mSbmM3aB3TIQ",
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:48.260298Z",
     "start_time": "2023-12-10T13:29:48.256313Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
   ],
   "metadata": {
    "id": "08boFe2k3YJm",
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:49.972429Z",
     "start_time": "2023-12-10T13:29:49.967388Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# To_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "#     roberta_model_name = 'roberta-base'\n",
    "#     num_classes = 2\n",
    "#     max_length = 128\n",
    "#     batch_size = 16\n",
    "#     num_epochs = 4\n",
    "#     learning_rate = 2e-5\n",
    "\n",
    "def to_train(model_name,num_classes,\n",
    "             train_texts, train_labels, val_texts, val_labels,\n",
    "             max_length=256, batch_size=16, num_epochs=4, learning_rate=2e-5):\n",
    "    \n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "    model = RobertaClassifier(model_name, num_classes).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        accuracy, report = evaluate(model, val_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T13:29:52.037574Z",
     "start_time": "2023-12-10T13:29:52.033992Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Roberta_binary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f99052a42d3e44ecb2d4727517397943"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/875 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b31603b0dae441aa63beb420812aef9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3833\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a2bb7af6ffb4c28981e4db5c8d4b28e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      1514\n",
      "           1       0.80      0.62      0.70       486\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.84      0.79      0.81      2000\n",
      "weighted avg       0.87      0.87      0.86      2000\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/875 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d6d116f55df4824af5877f82d21ac63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2554\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a484dbafbd14eefa1c6f056172a7b47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92      1514\n",
      "           1       0.81      0.62      0.70       486\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.85      0.79      0.81      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/875 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c52e75250ac4c0eb839c45c2573debb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.1655\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff30ceb62d5d4ce1ba88362fbcba61de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8770\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      1514\n",
      "           1       0.79      0.67      0.73       486\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.85      0.81      0.82      2000\n",
      "weighted avg       0.87      0.88      0.87      2000\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/875 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6892636671574bcc9bd8092f2d30b74a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0908\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/125 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2d71a08b5064869b2aac043ab4c0a1c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      1514\n",
      "           1       0.77      0.69      0.73       486\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.84      0.81      0.83      2000\n",
      "weighted avg       0.87      0.88      0.87      2000\n"
     ]
    }
   ],
   "source": [
    "roberta_base_binary = to_train(model_name = 'roberta-base', num_classes= 2,\n",
    "                               train_texts = train_texts_binary, train_labels = train_labels_binary,\n",
    "                               val_texts = dev_texts_binary, val_labels = dev_labels_binary,\n",
    "                               num_epochs = 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:06:12.259753Z",
     "start_time": "2023-12-10T11:03:27.716849Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Roberta_multi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# print('dev')\n",
    "# print(Counter(dev_labels_four_categories))\n",
    "# print('train')\n",
    "# print(Counter(train_labels_four_categories))\n",
    "# print('test')\n",
    "# print(Counter(test_labels_four_categories))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:06:12.296687Z",
     "start_time": "2023-12-10T12:06:12.291567Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# train_texts_three_categories = []\n",
    "# train_labels_three_categories = []\n",
    "# dev_texts_three_categories = []\n",
    "# dev_labels_three_categories = []\n",
    "# \n",
    "# for i in range(len(train_texts_four_categories)):\n",
    "#     if train_labels_four_categories[i] == 4 or train_labels_four_categories[i] == 3:\n",
    "#         continue\n",
    "#     train_texts_three_categories.append(train_texts_four_categories[i])\n",
    "#     train_labels_three_categories.append(train_labels_four_categories[i])\n",
    "#     \n",
    "# for i in range(len(dev_texts_four_categories)):\n",
    "#     if dev_labels_four_categories[i] == 4 or dev_labels_four_categories[i] == 3:\n",
    "#         continue\n",
    "#     dev_texts_three_categories.append(dev_texts_four_categories[i])\n",
    "#     dev_labels_three_categories.append(dev_labels_four_categories[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:06:12.298445Z",
     "start_time": "2023-12-10T12:06:12.294055Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Counter(train_labels_three_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:06:12.305885Z",
     "start_time": "2023-12-10T12:06:12.301054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Counter(train_labels_four_categories)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:06:12.306260Z",
     "start_time": "2023-12-10T12:06:12.303827Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0690ba580c9a4a639ba56f92c9e7d021"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6165d771b6034ccaabd40f3beef6c277"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.0812\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5649aad5080040f8a16293ae35f8f08b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5720\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.55      0.59        44\n",
      "           1       0.58      0.77      0.66       227\n",
      "           2       0.70      0.26      0.38       167\n",
      "           3       0.41      0.73      0.53        48\n",
      "\n",
      "    accuracy                           0.57       486\n",
      "   macro avg       0.59      0.58      0.54       486\n",
      "weighted avg       0.61      0.57      0.55       486\n",
      "\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "860932971f1540539d47e6442d6dca0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.8396\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88385e2612b648be8bd6c5f4c18bfaa8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.77      0.62        44\n",
      "           1       0.62      0.86      0.72       227\n",
      "           2       0.75      0.28      0.41       167\n",
      "           3       0.71      0.60      0.65        48\n",
      "\n",
      "    accuracy                           0.63       486\n",
      "   macro avg       0.65      0.63      0.60       486\n",
      "weighted avg       0.66      0.63      0.60       486\n",
      "\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "749dd0e089864c7a8397b0f9f206cf52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6296\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f04bc79f57434badb12cc398863750e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.61      0.67        44\n",
      "           1       0.68      0.68      0.68       227\n",
      "           2       0.55      0.67      0.61       167\n",
      "           3       0.77      0.35      0.49        48\n",
      "\n",
      "    accuracy                           0.64       486\n",
      "   macro avg       0.69      0.58      0.61       486\n",
      "weighted avg       0.65      0.64      0.64       486\n",
      "\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93b138542a5a443a917ee1861cdc9d08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.4209\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0504a85373374c7bbe46238c02eadf88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70        44\n",
      "           1       0.65      0.81      0.72       227\n",
      "           2       0.65      0.44      0.53       167\n",
      "           3       0.63      0.56      0.59        48\n",
      "\n",
      "    accuracy                           0.65       486\n",
      "   macro avg       0.66      0.63      0.64       486\n",
      "weighted avg       0.65      0.65      0.64       486\n",
      "\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d86d5750ae4c4272a76feb2da83485a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2565\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0061ce03bef4aafa7efba25ffe3fec5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.70      0.67        44\n",
      "           1       0.69      0.74      0.71       227\n",
      "           2       0.65      0.58      0.61       167\n",
      "           3       0.63      0.60      0.62        48\n",
      "\n",
      "    accuracy                           0.67       486\n",
      "   macro avg       0.65      0.66      0.65       486\n",
      "weighted avg       0.67      0.67      0.66       486\n",
      "\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d3d9104c6ec4a5cb45c44d6cb3f9f69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.1347\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4daa1a7f757445d08b8a3b0ce5713b42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.68      0.70        44\n",
      "           1       0.65      0.80      0.72       227\n",
      "           2       0.65      0.49      0.56       167\n",
      "           3       0.63      0.46      0.53        48\n",
      "\n",
      "    accuracy                           0.65       486\n",
      "   macro avg       0.66      0.61      0.63       486\n",
      "weighted avg       0.65      0.65      0.64       486\n",
      "\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9add3939d6db42028a05cd7d72c0659f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0807\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03451274741444849da7d9c0d07388ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.67        44\n",
      "           1       0.68      0.78      0.72       227\n",
      "           2       0.63      0.56      0.59       167\n",
      "           3       0.66      0.52      0.58        48\n",
      "\n",
      "    accuracy                           0.66       486\n",
      "   macro avg       0.67      0.62      0.64       486\n",
      "weighted avg       0.66      0.66      0.66       486\n",
      "\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/213 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a455b65d10724f77aacc302bf30992de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0584\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/31 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89e21feac9584fe2ab2357b53c88e619"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.64      0.69        44\n",
      "           1       0.68      0.76      0.72       227\n",
      "           2       0.63      0.59      0.61       167\n",
      "           3       0.66      0.52      0.58        48\n",
      "\n",
      "    accuracy                           0.67       486\n",
      "   macro avg       0.68      0.63      0.65       486\n",
      "weighted avg       0.67      0.67      0.67       486\n"
     ]
    }
   ],
   "source": [
    "roberta_base_multi = to_train(model_name = 'roberta-base', num_classes= 4,\n",
    "                               train_texts = train_texts_four_categories, train_labels = train_labels_four_categories,\n",
    "                               val_texts = dev_texts_four_categories, val_labels = dev_labels_four_categories,\n",
    "                              num_epochs =  8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:00:55.081933Z",
     "start_time": "2023-12-10T13:29:58.775765Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(roberta_base_binary, \"models/roberta_base_binary.pt\")"
   ],
   "metadata": {
    "id": "7EZNfu6a5f6a",
    "ExecuteTime": {
     "end_time": "2023-12-10T12:48:03.190122Z",
     "start_time": "2023-12-10T12:48:01.912437Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "torch.save(roberta_base_multi, \"models/roberta_base_multi.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:01:18.565012Z",
     "start_time": "2023-12-10T14:01:17.658252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_sexist(text, model, tokenizer, device, max_length=512):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return preds.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-10T12:47:45.302429Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test sentiment prediction\n",
    "test_text = (\"I need to kill that bitch\")\n",
    "sexist = predict_sexist(test_text, roberta_base_binary, RobertaTokenizer.from_pretrained('roberta-base'), torch.device(\"mps\"))\n",
    "category = predict_sexist(test_text, roberta_base_multi, RobertaTokenizer.from_pretrained('roberta-base'), torch.device(\"mps\")) if sexist else None\n",
    "print(test_text)\n",
    "print(f\"Predicted: {sexist} -> {category}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-10T12:47:45.302758Z"
    }
   }
  }
 ]
}
