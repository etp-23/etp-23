{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# !pip install emoji"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:21.999036Z",
     "start_time": "2023-11-12T19:30:21.861892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdOp8bpc05xi",
    "outputId": "a1c0f01f-a15e-4d42-9643-ec3977bfe771",
    "ExecuteTime": {
     "end_time": "2023-11-12T21:06:45.398880Z",
     "start_time": "2023-11-12T21:06:43.728572Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/louys/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/louys/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,\n",
    "                 emoji = None, #None, rm, translate\n",
    "                 urls_users_remover = False, #False, True\n",
    "                 lowercaser = False, #False, True\n",
    "                 punctuation_remover = False, #None,\n",
    "                 spell_checker = False, #False, True\n",
    "                 stopwords_remover = False, #False, True\n",
    "                 word_transformer = None, #lemmatization, stemming\n",
    "                 ):\n",
    "        self.emoji = emoji\n",
    "        self.urls_users_remover = urls_users_remover\n",
    "        self.lowercaser = lowercaser\n",
    "        self.spell_checker = spell_checker\n",
    "        self.punctuation_remover = punctuation_remover\n",
    "        self.stopwords_remover = stopwords_remover\n",
    "        self.word_transformer = word_transformer\n",
    "\n",
    "    def process(self, data:list)->list:\n",
    "        if self.emoji:\n",
    "            data = [self.de_emoji(text) for text in tqdm(data)]\n",
    "\n",
    "        if self.urls_users_remover:\n",
    "          data = [self.remove_urls_users(text) for text in tqdm(data)]\n",
    "\n",
    "        if self.lowercaser:\n",
    "            data = [self.lowercase(text) for text in tqdm(data)]\n",
    "\n",
    "        if self.spell_checker:\n",
    "            data = [self.correct_spelling(text) for text in tqdm(data)]\n",
    "\n",
    "        if self.punctuation_remover:\n",
    "            data = [self.remove_punctuations(text) for text in tqdm(data)]\n",
    "\n",
    "        if self.stopwords_remover:\n",
    "            data = [self.remove_stopwords(text) for text in tqdm(data)]\n",
    "\n",
    "        if self.word_transformer:\n",
    "            data = [self.word_transform(text) for text in tqdm(data)]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def de_emoji(self,text:str) -> str:\n",
    "        if self.emoji == 'rm':\n",
    "            cleaned_text = emoji.replace_emoji(text,'')\n",
    "        elif self.emoji == 'translate':\n",
    "            cleaned_text = emoji.demojize(text, delimiters=('', ''), language= 'en')\n",
    "            cleaned_text = cleaned_text.replace('_',' ')\n",
    "        return cleaned_text\n",
    "\n",
    "    def remove_urls_users(self, text:str) ->str:\n",
    "        cleaned_text = text.replace('[URL]','')\n",
    "        cleaned_text = cleaned_text.replace('[USER]','')\n",
    "        return cleaned_text\n",
    "\n",
    "    def remove_punctuations(self,text:str) -> str:\n",
    "        cleaned_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        return cleaned_text\n",
    "\n",
    "    def lowercase(self, text:str) -> str:\n",
    "        cleaned_text = text.lower()\n",
    "        return cleaned_text\n",
    "\n",
    "    def correct_spelling(self,text:str) -> str:\n",
    "        cleaned_text = TextBlob(text)\n",
    "        return cleaned_text.correct().string\n",
    "\n",
    "    def remove_stopwords(self,text:str) -> str:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
    "        return ' '.join(filtered_text)\n",
    "\n",
    "    def word_transform(self,text:str) -> str:\n",
    "        if self.word_transformer == 'lemmatization':\n",
    "            lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in nltk.tokenize.word_tokenize(text)]\n",
    "            return \" \".join(lemmatized_words)\n",
    "        elif self.word_transformer == 'stemming':\n",
    "            stemmer = nltk.stem.PorterStemmer()\n",
    "            stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
    "            return \" \".join(stemmed_words)"
   ],
   "metadata": {
    "id": "XCOZUmAw06T7",
    "ExecuteTime": {
     "end_time": "2023-11-12T21:06:47.095623Z",
     "start_time": "2023-11-12T21:06:47.091842Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/edos_labelled_aggregated.csv')"
   ],
   "metadata": {
    "id": "IIuLCM0K08Vn",
    "ExecuteTime": {
     "end_time": "2023-11-12T21:06:49.573633Z",
     "start_time": "2023-11-12T21:06:49.512877Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "preprocessor = Preprocessor(emoji = 'translate',\n",
    "                            urls_users_remover = True,\n",
    "                            punctuation_remover = True,\n",
    "                            word_transformer= 'stemming')\n",
    "data = list(df['text'])\n",
    "text_preprocessed = preprocessor.process(data)\n",
    "df['text_preprocessed'] = text_preprocessed"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wS32KtW1FLL",
    "outputId": "55c38a06-029b-48c9-ad88-a248be7ef4f5",
    "ExecuteTime": {
     "end_time": "2023-11-12T21:07:09.312217Z",
     "start_time": "2023-11-12T21:07:05.749999Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:00<00:00, 22384.79it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:00<00:00, 3183170.04it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:00<00:00, 405893.84it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [00:02<00:00, 7701.32it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                      rewire_id  \\\n0       sexism2022_english-9609   \n1      sexism2022_english-16993   \n2      sexism2022_english-13149   \n3      sexism2022_english-13021   \n4        sexism2022_english-966   \n...                         ...   \n19995   sexism2022_english-5228   \n19996  sexism2022_english-10140   \n19997   sexism2022_english-9726   \n19998  sexism2022_english-13365   \n19999   sexism2022_english-3523   \n\n                                                    text label_sexist  \\\n0      In Nigeria, if you rape a woman, the men rape ...   not sexist   \n1                                Then, she's a keeper. üòâ   not sexist   \n2      This is like the Metallica video where the poo...   not sexist   \n3                                                 woman?   not sexist   \n4                         I bet she wished she had a gun   not sexist   \n...                                                  ...          ...   \n19995     girls really get fucked almost every weekend ?   not sexist   \n19996  The hatred for moslems are Reasonable and Just...   not sexist   \n19997                 Now this is a woman who gets it. üëÜ   not sexist   \n19998  ‚ÄúAmerican Idol‚Äù finalist [USER] said nothing i...   not sexist   \n19999      this bi--th should be stoped she's the rapist       sexist   \n\n      label_category                        label_vector  split  \\\n0               none                                none    dev   \n1               none                                none  train   \n2               none                                none  train   \n3               none                                none  train   \n4               none                                none    dev   \n...              ...                                 ...    ...   \n19995           none                                none  train   \n19996           none                                none  train   \n19997           none                                none  train   \n19998           none                                none  train   \n19999  2. derogation  2.2 aggressive and emotive attacks   test   \n\n                                       text_preprocessed  \n0      in nigeria if you rape a woman the men rape yo...  \n1                            then she a keeper wink face  \n2      thi is like the metallica video where the poor...  \n3                                                  woman  \n4                           i bet she wish she had a gun  \n...                                                  ...  \n19995          girl realli get fuck almost everi weekend  \n19996  the hatr for moslem are reason and justifi the...  \n19997  now thi is a woman who get it backhand index p...  \n19998  ‚Äúamerican idol‚Äù finalist said noth is go to st...  \n19999            thi bith should be stope she the rapist  \n\n[20000 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rewire_id</th>\n      <th>text</th>\n      <th>label_sexist</th>\n      <th>label_category</th>\n      <th>label_vector</th>\n      <th>split</th>\n      <th>text_preprocessed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sexism2022_english-9609</td>\n      <td>In Nigeria, if you rape a woman, the men rape ...</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>dev</td>\n      <td>in nigeria if you rape a woman the men rape yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sexism2022_english-16993</td>\n      <td>Then, she's a keeper. üòâ</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>train</td>\n      <td>then she a keeper wink face</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sexism2022_english-13149</td>\n      <td>This is like the Metallica video where the poo...</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>train</td>\n      <td>thi is like the metallica video where the poor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sexism2022_english-13021</td>\n      <td>woman?</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>train</td>\n      <td>woman</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sexism2022_english-966</td>\n      <td>I bet she wished she had a gun</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>dev</td>\n      <td>i bet she wish she had a gun</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>sexism2022_english-5228</td>\n      <td>girls really get fucked almost every weekend ?</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>train</td>\n      <td>girl realli get fuck almost everi weekend</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>sexism2022_english-10140</td>\n      <td>The hatred for moslems are Reasonable and Just...</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>train</td>\n      <td>the hatr for moslem are reason and justifi the...</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>sexism2022_english-9726</td>\n      <td>Now this is a woman who gets it. üëÜ</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>train</td>\n      <td>now thi is a woman who get it backhand index p...</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>sexism2022_english-13365</td>\n      <td>‚ÄúAmerican Idol‚Äù finalist [USER] said nothing i...</td>\n      <td>not sexist</td>\n      <td>none</td>\n      <td>none</td>\n      <td>train</td>\n      <td>‚Äúamerican idol‚Äù finalist said noth is go to st...</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>sexism2022_english-3523</td>\n      <td>this bi--th should be stoped she's the rapist</td>\n      <td>sexist</td>\n      <td>2. derogation</td>\n      <td>2.2 aggressive and emotive attacks</td>\n      <td>test</td>\n      <td>thi bith should be stope she the rapist</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows √ó 7 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T21:07:24.567901Z",
     "start_time": "2023-11-12T21:07:24.549705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "id": "qK122ibS1Lht",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.832325Z",
     "start_time": "2023-11-12T19:30:22.815285Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# train_data.to_csv('/content/train.csv', index=False)\n",
    "# test_data.to_csv('/content/test.csv', index=False)"
   ],
   "metadata": {
    "id": "u5fHCeXh1NOY",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.861997Z",
     "start_time": "2023-11-12T19:30:22.817810Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAlU-wxg1z9-",
    "outputId": "3f725748-30fd-466f-d228-a1c9bf952765",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.862271Z",
     "start_time": "2023-11-12T19:30:22.819818Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "777u8TG11PYp",
    "ExecuteTime": {
     "end_time": "2023-11-12T21:14:03.421237Z",
     "start_time": "2023-11-12T21:14:01.710514Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data(df):\n",
    "    texts = df['text_preprocessed'].tolist()\n",
    "    labels = [1 if (label == True) else 0 for label in df['label_sexist'].tolist()]\n",
    "    return texts, labels"
   ],
   "metadata": {
    "id": "DBrSru0z7gzV",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.862412Z",
     "start_time": "2023-11-12T19:30:22.825017Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "texts, labels = load_data(df)"
   ],
   "metadata": {
    "id": "6nUilRVM8BOY",
    "ExecuteTime": {
     "end_time": "2023-11-12T20:07:25.428004Z",
     "start_time": "2023-11-12T20:07:25.417242Z"
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
   ],
   "metadata": {
    "id": "g251hIrU1yM4",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.864742Z",
     "start_time": "2023-11-12T19:30:22.829984Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ],
   "metadata": {
    "id": "E7BC_lx73ISo",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.865706Z",
     "start_time": "2023-11-12T19:30:22.834930Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ],
   "metadata": {
    "id": "mSbmM3aB3TIQ",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.865970Z",
     "start_time": "2023-11-12T19:30:22.839681Z"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
   ],
   "metadata": {
    "id": "08boFe2k3YJm",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.866038Z",
     "start_time": "2023-11-12T19:30:22.842625Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_sexist(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "    return True if preds.item() == 1 else False"
   ],
   "metadata": {
    "id": "j0LOx_Ba3cMj",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:41:43.740084Z",
     "start_time": "2023-11-12T19:41:43.718747Z"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up parameters\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "num_epochs = 4\n",
    "learning_rate = 2e-5"
   ],
   "metadata": {
    "id": "c6eLtjLy3icT",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.866136Z",
     "start_time": "2023-11-12T19:30:22.848319Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "id": "VsMLpUYF3uyE",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.866183Z",
     "start_time": "2023-11-12T19:30:22.850381Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ChCW_q3u6cIf",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:22.866228Z",
     "start_time": "2023-11-12T19:30:22.856081Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "id": "QmLcdTsi4gh1",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:24.447058Z",
     "start_time": "2023-11-12T19:30:22.857703Z"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b16b5bf074f94810a2551d2e40a549c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16fe08edf0bd49f5b6f5baf879d62fd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bab6582e4e5e4eceaaea4b6e078f542a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ],
   "metadata": {
    "id": "IAEFVguR4l-T",
    "ExecuteTime": {
     "end_time": "2023-11-12T21:13:46.681437Z",
     "start_time": "2023-11-12T21:13:46.277954Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m cuda\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m BERTClassifier(bert_model_name, num_classes)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSFRj0HI4qRd",
    "outputId": "595c50dd-384e-447c-8ac2-87154c9e3dc2",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:36.366727Z",
     "start_time": "2023-11-12T19:30:36.331802Z"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louys/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6VKnSkU1-JYw",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:30:36.366923Z",
     "start_time": "2023-11-12T19:30:36.364566Z"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train(model, train_dataloader, optimizer, scheduler, device)\n",
    "    accuracy, report = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkJK6wY94w7s",
    "outputId": "034d7a36-762d-47bd-c8eb-9a75bd7ee340",
    "ExecuteTime": {
     "end_time": "2023-11-12T20:09:42.835521Z",
     "start_time": "2023-11-12T20:07:42.946122Z"
    }
   },
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:37<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       145\n",
      "           1       0.62      0.58      0.60        55\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.73      0.72      0.73       200\n",
      "weighted avg       0.78      0.79      0.78       200\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:25<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       145\n",
      "           1       0.62      0.58      0.60        55\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.73      0.72      0.73       200\n",
      "weighted avg       0.78      0.79      0.78       200\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:25<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       145\n",
      "           1       0.62      0.58      0.60        55\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.73      0.72      0.73       200\n",
      "weighted avg       0.78      0.79      0.78       200\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:24<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       145\n",
      "           1       0.62      0.58      0.60        55\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.73      0.72      0.73       200\n",
      "weighted avg       0.78      0.79      0.78       200\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# torch.save(model.state_dict(), \"berta_classifier.pth\")"
   ],
   "metadata": {
    "id": "7EZNfu6a5f6a",
    "ExecuteTime": {
     "end_time": "2023-11-12T19:38:51.848957Z",
     "start_time": "2023-11-12T19:38:49.076189Z"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "men should be in the kitchen.\n",
      "Predicted: False\n"
     ]
    }
   ],
   "source": [
    "test_text = (\"men should be in the kitchen.\")\n",
    "sexist = predict_sexist(test_text, model, tokenizer, device)\n",
    "print(test_text)\n",
    "print(f\"Predicted: {sexist}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T19:46:27.353753Z",
     "start_time": "2023-11-12T19:46:26.097696Z"
    }
   }
  }
 ]
}
