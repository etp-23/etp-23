{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58de3512c343787",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cab0df7a64ce6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:50.774262Z",
     "start_time": "2023-12-11T00:12:49.495350Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be3f62cdd518300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:50.801553Z",
     "start_time": "2023-12-11T00:12:50.775881Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/annotation task/comMid2k_Shuffled_no_none.tsv',sep='\\t')\n",
    "df_dict = df.apply(lambda x:{\"x\":x['text'],\"y\":x['mitigated_text']}, axis=1)\n",
    "df = pd.DataFrame(df_dict, columns=['mitigation'])\n",
    "raw_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9187334bf3c5bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:51.820047Z",
     "start_time": "2023-12-11T00:12:50.801175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/xsb_n5f52wb0yymcyysnq0980000gn/T/ipykernel_94634/3247265623.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4428abc0a776fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:51.825650Z",
     "start_time": "2023-12-11T00:12:51.822479Z"
    }
   },
   "outputs": [],
   "source": [
    "split_datasets = raw_dataset.train_test_split(test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:51.830604Z",
     "start_time": "2023-12-11T00:12:51.825862Z"
    }
   },
   "id": "a2ffc39cfdc60946"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd4e7e3fdef7661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:36:05.122804Z",
     "start_time": "2023-12-11T07:36:05.116845Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 22\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "learning_rate = 1e-04\n",
    "source_tag = \"x\"\n",
    "target_tag = \"y\"\n",
    "batch_size = 8\n",
    "model_checkpoint = 'facebook/bart-base'\n",
    "output_dir = \"models/m3bart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3168992fd42fba6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:57.822306Z",
     "start_time": "2023-12-11T00:12:55.703296Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f38c22a23c933a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:57.828052Z",
     "start_time": "2023-12-11T00:12:57.824193Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex[source_tag] for ex in examples[\"mitigation\"]]\n",
    "    targets = [ex[target_tag] for ex in examples[\"mitigation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ba2719e793b292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:12:58.003096Z",
     "start_time": "2023-12-11T00:12:57.843426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1658 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72f5c4ee39da48fc83b5798d709a8922"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louys/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3862: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/293 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "277bcc1ab5d44beba8f26b8162f7da92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(preprocess_function, batched = True, remove_columns=split_datasets[\"train\"].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af6826048d37a7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:02.575122Z",
     "start_time": "2023-12-11T00:12:58.194531Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "device = torch.device(\"mps\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "716a3323d0e1013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:02.591561Z",
     "start_time": "2023-12-11T00:13:02.575753Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:02.591717Z",
     "start_time": "2023-12-11T00:13:02.578617Z"
    }
   },
   "id": "e325d1db291a780b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:02.595107Z",
     "start_time": "2023-12-11T00:13:02.581044Z"
    }
   },
   "id": "9977fc441af955c6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:03.052354Z",
     "start_time": "2023-12-11T00:13:03.047548Z"
    }
   },
   "id": "540111b01f7a395d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:03.612087Z",
     "start_time": "2023-12-11T00:13:03.605364Z"
    }
   },
   "id": "97033a79ca867dc3"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = epochs\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:04.007182Z",
     "start_time": "2023-12-11T00:13:04.004628Z"
    }
   },
   "id": "edc3d2874c63a5c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T00:13:06.109139Z",
     "start_time": "2023-12-11T00:13:06.107370Z"
    }
   },
   "id": "3905eb9ac9b60e54"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4160 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9998fa6bccdd4caab3025e554f845cf6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fac97f9667f42129c61772ae74d5ebd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louys/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:822: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
      "/Users/louys/anaconda3/lib/python3.11/site-packages/transformers/generation/beam_search.py:372: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  sent_lengths_max = sent_lengths.max().item() + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, BLEU score: 19.32\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a84b851b524f40e39b378b71297892bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, BLEU score: 17.58\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d43357f41bed4a56a44502eeb95b61e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, BLEU score: 17.46\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff90e8ae6179486ab8d5adf9c7879358"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, BLEU score: 17.78\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "910ae416a5be4d55a14775a465bc833b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, BLEU score: 17.47\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8583453bebf24666a383fc08da08e8b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, BLEU score: 16.81\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d719d4f778574fe3a65fe929e73ee7d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, BLEU score: 17.40\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dde164bb626d4327bc279b7b6950ca9d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, BLEU score: 17.19\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "450201fd2ed541a1b492098a547cd26f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, BLEU score: 16.17\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0480609b39e2463bbf295273797fe24a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, BLEU score: 15.75\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4384b1fd6a904d48ab5d6edf7e479462"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, BLEU score: 16.34\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22ac34ca66b74751b471de035c54f309"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, BLEU score: 15.73\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "940d169861624cfa8cf0f419108e971f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, BLEU score: 16.46\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02852e00c9b84d168497a8312333fe35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, BLEU score: 15.48\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3682f2b0af3484c90920a63aeceebdf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, BLEU score: 17.07\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "460a12b5de1d4426b6b7b926ffac6469"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, BLEU score: 17.08\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a71057bedefb42a083e54058ed36ea3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, BLEU score: 17.41\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c748f45bdf6d48ccae5e4531224d7af8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, BLEU score: 17.65\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97b98f950bc34e0698e412c3e8b02211"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, BLEU score: 17.31\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/37 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45c64c116caa408cb40050a6e877bbbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, BLEU score: 17.32\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T04:04:16.842988Z",
     "start_time": "2023-12-11T00:13:07.535946Z"
    }
   },
   "id": "d3111294dbc6915a"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# accelerator.wait_for_everyone()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:20:39.871718Z",
     "start_time": "2023-12-11T07:20:38.304884Z"
    }
   },
   "id": "bc5343ae5cd80a0e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "('models/m3bart/tokenizer_config.json',\n 'models/m3bart/special_tokens_map.json',\n 'models/m3bart/vocab.json',\n 'models/m3bart/merges.txt',\n 'models/m3bart/added_tokens.json',\n 'models/m3bart/tokenizer.json')"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:24:13.679010Z",
     "start_time": "2023-12-11T07:24:13.621335Z"
    }
   },
   "id": "5132ed77f8d62fad"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "model_to_generate = AutoModelForSeq2SeqLM.from_pretrained(\"models/m3bart\")\n",
    "model_to_generate = model_to_generate.to('mps')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:27:31.080143Z",
     "start_time": "2023-12-11T07:27:11.517488Z"
    }
   },
   "id": "1bfb6201c80386e2"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louys/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1353: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She may not have been well-suited for motherhood, as many career-']\n"
     ]
    }
   ],
   "source": [
    "test_examples = \"She would have made a poor mother. As most career women are. \"\n",
    "\n",
    "inputs = tokenizer(test_examples, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(model_to_generate.device)\n",
    "attention_mask = inputs.attention_mask.to(model_to_generate.device)\n",
    "\n",
    "outputs = model_to_generate.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "output_str = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "print(output_str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:33:35.972554Z",
     "start_time": "2023-12-11T07:33:33.113912Z"
    }
   },
   "id": "bb59c2416f0e76a8"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louys/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1353: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'translation_text': \"Using derogatory language in interactions isn't appropriate.\"}]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T07:25:24.183998Z",
     "start_time": "2023-12-11T07:25:23.537579Z"
    }
   },
   "id": "a7290df75326a9a1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# args = Seq2SeqTrainingArguments(\n",
    "#     model_name,\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=1,\n",
    "#     predict_with_generate=True,\n",
    "#     # fp16=True,\n",
    "#     # push_to_hub=True,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-10T20:45:41.391891Z"
    }
   },
   "id": "b241d3f024307549"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"models\",\n",
    "#     num_train_epochs=epochs,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     learning_rate=learning_rate,\n",
    "#     warmup_steps=200,\n",
    "#     weight_decay=0.001,\n",
    "#     predict_with_generate=True,\n",
    "#     logging_dir=\"logs\",\n",
    "#     logging_steps=10,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_total_limit=3,\n",
    "#     generation_max_length=max_target_length,\n",
    "#     generation_num_beams=2,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"rouge-1\",\n",
    "#     # disable_tqdm=True,\n",
    "#     report_to=\"all\"\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-10T20:45:41.411060Z"
    }
   },
   "id": "11a5d390d632eaea"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2e11a9e9d367243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:45:41.423905Z",
     "start_time": "2023-12-10T20:45:41.421340Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# def postprocess_text(preds, labels):\n",
    "#     preds = [pred.strip() for pred in preds]\n",
    "#     labels = [[label.strip()] for label in labels]\n",
    "# \n",
    "#     return preds, labels\n",
    "# \n",
    "# def compute_metrics(eval_preds):\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "# \n",
    "# #    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "# \n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "# \n",
    "#     # Some simple post-processing\n",
    "#     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "# \n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     result = {\"bleu\": result[\"score\"]}\n",
    "# \n",
    "#     # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "#     # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "#     # result = {k: round(v, 4) for k, v in result.items()}\n",
    "#     return result\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c893c8c929c1ed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:45:41.451762Z",
     "start_time": "2023-12-10T20:45:41.428917Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainer = Seq2SeqTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets['test'],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d337afc1fa4937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T20:46:03.328560Z",
     "start_time": "2023-12-10T20:45:44.350248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='975' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/975 00:10 < 18:50, 0.85 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/49 00:14 < 11:30, 0.07 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c501097f8b20c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:51:58.634210Z",
     "start_time": "2023-12-08T14:51:56.434726Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainer.save_model(\"models/bart-base-text-style-transfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429078c4dcf4ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:52:26.401286Z",
     "start_time": "2023-12-08T14:52:10.106132Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_to_generate = AutoModelForSeq2SeqLM.from_pretrained(\"models/bart-base-text-style-transfer\")\n",
    "# model_to_generate = model_to_generate.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed38e0ace0cd39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T19:51:45.156773Z",
     "start_time": "2023-12-10T19:51:40.836817Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_examples = \"Because she's a skank\"\n",
    "# \n",
    "# inputs = tokenizer(test_examples, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "# input_ids = inputs.input_ids.to(model_to_generate.device)\n",
    "# attention_mask = inputs.attention_mask.to(model_to_generate.device)\n",
    "# \n",
    "# outputs = model_to_generate.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "# output_str = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "# print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c69858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
